{"cells":[{"metadata":{"_uuid":"a3720af58141457d2afbc21ed785af69bbf9cdeb"},"cell_type":"markdown","source":"**Forecasting insurance price for customers using Regression techniques**\n\nClick here for data source - [Data source](https://www.kaggle.com/mirichoi0218/insurance)\n\nThe dataset is a collection of medical cost prices for 1338 instances. The objective is to predict the charges for customers based on certain information available about them. **Feature set is as follows: **\n\n* age: age of primary beneficiary\n* sex: insurance contractor gender, female, male\n* bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n* children: Number of children covered by health insurance / Number of dependents\n* smoker: Smoking status (whether smokes or not)\n* region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n* charges: Individual medical costs billed by health insurance\n\nPredicting the charges will require application of regression algorithms such as Random Forest Regressor and Linear Regression, etc. Before diving in to generation of model, there are some steps necessary to render our data into model understandable and usable format. Also, to understand the type of data we are dealing with, studying its features and statistical analysis of data is required. \n\n**Some of the steps required are -**\n* Data description - to view and understand how the data looks like, what features exist - their datatypes and values they hold.\n* Target variable - The most important aspect of the data. Charges is our target (to predict) and we see how it is distrubuted in the data.\n* Data cleaning and pre-processing - finding and handling missing values, checking for valid column names and valid entries for those column, converting data-types of columns in to model acceptable formats and dealing with categorical variables (by generating dummy variables or by updating exisiting features with binary values).\n* Data visualization - To generate hidden insights from the data. For example, smokers are charged higher charges than non-smokers.\n**Visualization is also required to figure out which features are responsible for changes in the target variable. This is called feature correlation.** \n* Prepare data, model generation and testing -\nThis is the part where Machine learning comes in to picture. Data is divided into training and testing sets. Models are produced by learning training data and finally, their performance is evaluated on testing/unseen data.\nA good model is capable of accuractely predicting target for unseen instances. \nA poor model maybe a result of excessive parameter tuning (adjusting parameters to perform well precisely on training data), over-fitting (model learns training data too much and does not understand how to deal with new/unseen feature values) or due to structure of data itself (extremely noisy, messy, highly uncorrelated, unevenly distributed, etc.)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Read the data\ndata = pd.read_csv(\"../input/insurance.csv\")\n\n# See how top 5 rows of the data look like.\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edb6e72261fa5eebc3c534227a70fb2b647807c2"},"cell_type":"code","source":"# How bottom 5 rows look like.\ndata.tail() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f5dfbbe505adf4e736ed7998aa0937cf7a0eda0"},"cell_type":"code","source":"# Generate statistical summary of the data's numerical features\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7934872f877222b9f1eca7ba31931beea35b26c2"},"cell_type":"markdown","source":"**Information from above stats - **\n\nAverage age of customers is about 39 years with maximum age of 64 years and they have one child on an average with minimum of no child and maximum of 5 children.\n75% of observations show 51 years of age and 2 children.\nThe charges for insurance on an average is 13270.42 units with 75% obseravtions close to 16639.91 units."},{"metadata":{"trusted":true,"_uuid":"406c86e054506f791372ad70786bd8f93d194543","scrolled":true},"cell_type":"code","source":"# View all column names and their respective data types\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ded274593c28eb463cff572446bc0b4494a5699"},"cell_type":"code","source":"# Check for missing values\nprint(data.isnull().sum())\n\n#All zeros show that there is no missing value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42255c3b51d7174f3087297a112a401e570117e8"},"cell_type":"code","source":"#-------------------- DATA VISUALIZATION -------------------------\n# Visualize distribution of values for target variable - 'charges'\nplt.figure(figsize=(6,6))\nplt.hist(data.charges, bins = 'auto', color = 'purple')\nplt.xlabel(\"charges ->\")\nplt.title(\"Distribution of charges values :\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9638c4402130b13a2d8de923402377b118cc58e"},"cell_type":"markdown","source":"**What we know about target variable?**\n* It is unevenly distrubuted.\n* Most beneficiaries are charged between 1000 to 10,000 units.\n* Very few are charged above 50,000.\n* We already know from statistical data description above that mean is 13270.42 (close to lower limit of target range), which inclines the data towards the left of the distribution."},{"metadata":{"trusted":true,"_uuid":"2fe8ca270a0585b75b3ff3322086f83ebe0efd05"},"cell_type":"code","source":"# Generate Box-plots to check for outliers and relation of each feature with 'charges'\ncols = ['age', 'children', 'sex', 'smoker', 'region']\nfor col in cols:\n    plt.figure(figsize=(8,8))\n    sns.boxplot(x = data[col], y = data['charges'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a93c46166656de97f2a99fdea26be08a1241ed9b"},"cell_type":"markdown","source":"**Insights from boxplots generated above -**\n* As **age** increases, insurance cost increases. The plots show an increasing trend (with several small ranges for charges for some ages) in charges starting from around 1000 for age 18-19 to about 10,000 or so for customers with age near 60 \n    - This may be due to general medical assumption that younger people are more fit or possess robust immune system. \n     - Another reason could be the types of medical conditions covered by the insurance. If the insurance is designed to cover conditions likely to develop with growing age, charges will be higher for older age groups.\n* **Customers with 2 children** are charged highest when compared to others. Those with 5 or more children are charged less - This may be due to dominance of group with 2 or 3 children in the entire population.\n* Being **a male or female** have lesser impact on cost, even though range for males is larger than for females. That means, males are charged higher in several cases than maximum charges for females.\n* The plot shows a clear distribution pattern of high charges for beneficiaries who are **smokers** and considerably low costs for **non-smokers**.\n* **Region** does not show much correlation with charges, though, South-east region have larger range up to about 20,000 in its dsitribution of customer charges. - This could be due to medical costs being higher in the region, some pre-known environmental/physical hazards or because it is a well-developed area with higher costs of living. \n\n"},{"metadata":{"trusted":true,"_uuid":"e513548f5c9d2f75ebdba0af2ad71ce27525ccda","scrolled":true},"cell_type":"code","source":"# Converting categorical features' string values to int\n# Updating directly to binary because only two values exist\ndata.smoker = [1 if x == 'yes' else 0 for x in data.smoker]\ndata.sex = [1 if x == 'male' else 0 for x in data.sex]\n\n# Use pandas because multiple values exist for these columns.\ndata.region = pd.get_dummies(data.region)\ndata.charges = pd.to_numeric(data.charges)\ndata.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a37ef45e1e917ced68e51f3ed51982f72bf315f"},"cell_type":"code","source":"# Create Correlation matrix for all features of data.\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b316151b7a57b36fb5e4677a388c5df4ebc22ec0"},"cell_type":"code","source":"# Generate heatmap to visualize strong & weak correlations.\nsns.heatmap(data.corr())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"639ee622a2b5a2be32c55c2f483bab79beff4265"},"cell_type":"markdown","source":"* Above heatmap shows that there is **highest correlation between Charges and whether customer is a smoker** and **lowest correlation between Region and Charges**\n\nSince, there are only few features, it is feasible to generate pairplots for all of them. Otherwise, we would have only generated pairplots for features having high positive or negative correlation with the target variable."},{"metadata":{"trusted":true,"_uuid":"3ebdfafda39abd36cd381f60a93351002cdcbe37"},"cell_type":"code","source":"# Generate pairplots for all features because there are only 7 in all.\nsns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84ef16b1a6c3e2d67a2d86eeea6afe8d9eb2ea03"},"cell_type":"code","source":"#------------------- Prepare data for predictive regression models ----------------------------\ny = data.charges.values\nX = data.drop(['charges'], axis = 1)   # Drop the target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3db202f2e9478cdad47f5f2e07d7d45be9e4cef9"},"cell_type":"code","source":"# import scikit learn's built-in Machine learning libraries and functions\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport statsmodels.api as sm\n\n# Split using 20% for testing and 80% for training\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = None)\n\n# ----------------- PREDICTIVE MODELLING (Call the models to be used) -----------------------\nrf_reg = RandomForestRegressor(max_features = 'auto', bootstrap = True, random_state = None)\nlin_reg = LinearRegression(normalize = True)\nada_reg = AdaBoostRegressor()\n\n# R2-score is used here as a metric. Any other metric could be used instead by just importing \n# it from sklearn\n\n# Predict using Random Forest Regressor.\nrf_reg.fit(X_train, y_train)\npredtrainRF = rf_reg.predict(X_train)     # Prediction for train data\npredtestRF = rf_reg.predict(X_test)       # Prediction for test data\n\n# Compute R-squared score for both train and test data.\nprint(\"R2-score on train data:\", r2_score(y_train,predtrainRF))\nprint(\"R2-score on test data:\", r2_score(y_test, predtestRF))\n\n# Predict using Linear Regression\nlin_reg.fit(X_train, y_train)\npredtrainL = lin_reg.predict(X_train)\npredtestL = lin_reg.predict(X_test)\nprint(\"R2-score on train data:\",r2_score(y_train, predtrainL))\nprint(\"R2-score on test data:\",r2_score(y_test, predtestL))\n\n# Predict using XGBoost Regressor\nada_reg.fit(X_train, y_train)\npredtrainAda = ada_reg.predict(X_train)\npredtestAda = ada_reg.predict(X_test)\nprint(\"R2-score on train data:\",r2_score(y_train, predtrainAda))\nprint(\"R2-score on test data:\",r2_score(y_test, predtestAda))\n\n# ----------------- Using Ordinary Least Square from Statsmodel --------------------------------\n# -------- Allows to view full summary statistics along with p-value and F-statistics -----------\n# On Train data.\nX_newtrain = sm.add_constant(X_train)\nols_train = sm.OLS(y_train, X_newtrain)\nols_train_new = ols_train.fit()\nprint(ols_train_new.summary())\n\n# On Test data.\nX_newtest = sm.add_constant(X_test)\nols_test = sm.OLS(y_test, X_newtest)\nols_test_new = ols_test.fit()\nprint(ols_test_new.summary())   # Produce full statistical summary \n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ca50b959195a8948b4b59d91ef2b3dbd43fed67"},"cell_type":"markdown","source":"Please feel free to provide your comments/suggestions below and please do **upvote**, if you liked this work.*\n\nThank you and happy learning!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}